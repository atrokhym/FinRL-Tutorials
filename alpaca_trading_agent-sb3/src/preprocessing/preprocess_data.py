# alpaca_trading_agent/src/preprocessing/preprocess_data.py

import pandas as pd
import numpy as np
from stockstats import StockDataFrame as Sdf
import os
import logging
import argparse # Added argparse
import sys # Added sys

# Add src directory to sys.path to allow importing utils
SRC_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, SRC_DIR)

# Logging setup will be done explicitly using the shared utility
try:
    # Import both functions now
    from utils.logging_setup import configure_file_logging, add_console_logging
except ImportError:
     # Fallback if the utility somehow isn't found
    logging.basicConfig(
        level=logging.ERROR, format="%(asctime)s - %(levelname)s - %(message)s"
    )
    logging.error(
        "Failed to import logging setup functions from src.utils. Cannot configure logging."
    )
    # Define dummy functions to prevent NameErrors later if import fails
    def configure_file_logging(level): pass
    def add_console_logging(level): pass

# --- Configuration Loading ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
# SRC_DIR is already defined above

# Add config directory to sys.path
# import sys # Already imported above
sys.path.insert(0, CONFIG_DIR)

try:
    import settings
except ImportError as e:
    logging.error(f"Error importing configuration: {e}. Ensure config/settings.py exists.")
    sys.exit(1)

# --- Preprocessing Function ---
def preprocess_data(df: pd.DataFrame, is_live_trading: bool = False) -> pd.DataFrame | None:
    """
    Preprocesses the raw stock data fetched from Alpaca.
    - Calculates technical indicators using stockstats.
    - Calculates turbulence index (rolling std dev of returns).
    - Handles missing values.
    - Formats the DataFrame for FinRL environment.
    - NOTE: When using for live/incremental data, ensure the input `df`
      contains sufficient historical lookback rows for indicators to be calculated correctly.

     Args:
         df (pd.DataFrame): Raw data with columns like ['open', 'high', 'low', 'close', 'volume', 'tic'].
                            Index should be datetime. It should contain enough historical rows for indicator calculation.
         is_live_trading (bool, optional): If True, selects only the last row for prediction. Defaults to False.

     Returns:
         pd.DataFrame | None: Processed DataFrame ready for FinRL, or None if error.
     """
    # Define turbulence window
    TURBULENCE_WINDOW = 30

    logging.info(f"Starting preprocessing. Initial shape: {df.shape}")

    # Convert index to datetime if it's not already (it should be from fetch_data)
    # Handle potential 'date' column if index reset happened before calling
    if 'date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['date']):
         df['date'] = pd.to_datetime(df['date'])
         df = df.set_index('date', drop=False) # Set date as index but keep column
    elif not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index)
        df.index.name = 'date' # Name the index


    # Use StockDataFrame for indicator calculation
    # Ensure columns are named as expected by stockstats: open, high, low, close, volume
    # Our fetch_data script already lowercased them.
    try:
        stock = Sdf.retype(df.copy())
    except Exception as e:
         logging.error(f"Failed to retype DataFrame for StockStats: {e}")
         logging.error(f"Columns available: {df.columns.tolist()}")
         return None


    # Use indicators defined in settings
    # State features: close, high, low, trade_count, open, volume, vwap
    # Technical indicators: macd, rsi_14, cci_14, boll_ub, boll_lb
    indicators = settings.INDICATORS # Get base indicators list

    logging.info(f"Calculating base indicators: {indicators}")

    # Calculate base indicators defined in settings.INDICATORS
    for indicator in indicators:
        if indicator in df.columns: # Skip if already exists (e.g., price/volume)
            continue
        try:
            indicator_data = stock[indicator]
            df[indicator] = indicator_data
            logging.debug(f"Calculated {indicator}")
        except KeyError:
            logging.warning(f"Stockstats indicator '{indicator}' not found or failed calculation.")
        except Exception as e:
            logging.warning(f"Could not calculate indicator '{indicator}': {e}")

    # Handle potential missing values generated by indicators
    # Option 1: Forward fill (Handles NaNs from indicators at start)
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., if ffill couldn't fill from the start) with 0
    # df.fillna(0, inplace=True) # Delay filling remaining NaNs until after turbulence


    # --- Ensure 'date' and 'tic' columns exist ---
    if 'date' not in df.columns:
        # If 'date' is the index name, reset it
        if df.index.name == 'date':
            df = df.reset_index()
        else:
             # Attempt to find a date-like column if index wasn't 'date'
             date_col = next((col for col in df.columns if 'date' in col.lower()), None)
             if date_col and not pd.api.types.is_datetime64_any_dtype(df[date_col]):
                 df[date_col] = pd.to_datetime(df[date_col])
                 df = df.rename(columns={date_col: 'date'})
             elif not date_col:
                 logging.error(f"Could not find or create 'date' column. Columns: {df.columns.tolist()}")
                 return None

    if 'tic' not in df.columns:
         logging.error(f"'tic' column missing. Columns: {df.columns.tolist()}")
         return None


    # --- Calculate Turbulence Index ---
    try:
        # Calculate daily returns per tic (requires 'date' and 'tic' columns)
        df = df.sort_values(by=['tic', 'date']) # Sort for pct_change
        df['daily_return'] = df.groupby('tic')['close'].pct_change()

        # Calculate rolling standard deviation of returns per tic
        df['turbulence'] = df.groupby('tic')['daily_return'].transform(lambda x: x.rolling(window=TURBULENCE_WINDOW, min_periods=TURBULENCE_WINDOW).std())

        # Drop the intermediate daily_return column
        df.drop(columns=['daily_return'], inplace=True)
        logging.info(f"Calculated turbulence index (rolling {TURBULENCE_WINDOW}-day std dev).")
    except KeyError as e:
         logging.error(f"KeyError during turbulence calculation (likely missing 'date', 'tic', or 'close'): {e}")
         return None
    except Exception as e:
         logging.error(f"Error during turbulence calculation: {e}", exc_info=True)
         return None
    # --- End Turbulence Calculation ---


    # --- Handle NaNs from Turbulence and Fill Remaining ---
    # Forward fill again to propagate last valid turbulence value if needed
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., at the very start of the series for indicators/turbulence/vix) with 0
    df.fillna(0, inplace=True)


    # --- Select only the last row IF in live trading mode ---
    if is_live_trading:
        logging.info("Preprocessing for live trading: Selecting last row.")
        if not df.empty:
            df = df.sort_values(by=['date', 'tic']) # Ensure sorted before taking last
            # Select the last entry for each ticker based on the latest date
            df = df.loc[df.groupby('tic')['date'].idxmax()]
            df = df.reset_index(drop=True) # Reset index after selection
        else:
            logging.warning("DataFrame became empty before selecting last row.")
            return None
    else:
         logging.info("Preprocessing for training/backtesting: Using full timeseries.")

    # Sort by date and ticker - crucial for FinRL environments
    # This should happen *before* returning, even for live trading (though df is small then)
    if 'date' in df.columns and 'tic' in df.columns:
        df = df.sort_values(by=['date', 'tic']).reset_index(drop=True)
    else:
        # This check might be redundant now but keep for safety
        logging.error(f"Required columns ('date', 'tic') not found for final sorting. Columns: {df.columns.tolist()}")
        return None

    logging.info(f"Preprocessing finished. Final shape: {df.shape}")
    logging.info(f"Columns: {df.columns.tolist()}")

    if not df.empty:
        logging.info(f"Date range: {df['date'].min()} to {df['date'].max()}")
    else:
        logging.warning("Processed dataframe is empty.")


    # Check for any remaining NaN values
    if df.isnull().values.any():
        logging.warning("NaN values remain after preprocessing. Review handling.")
        logging.warning(f"Columns with NaN: {df.columns[df.isnull().any()].tolist()}")
        # Log rows with NaN for inspection (limit output)
        nan_rows = df[df.isnull().any(axis=1)]
        logging.warning(f"Sample rows with NaN:\n{nan_rows.head().to_string()}")


    return df

# Logging setup is handled by the calling script (main.py) or within __main__ here.
# --- Main Execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data Preprocessor for Alpaca Trading Agent")
    parser.add_argument(
        '--log-level',
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help="Set the logging level for the script."
    )
    parser.add_argument(
        '--train-end-date',
        type=str,
        default=None,
        help="Specify the end date for filtering data in YYYY-MM-DD format. Overrides settings.py TEST_END_DATE."
    )
    args = parser.parse_args()

    # --- Configure Logging for this script ---
    try:
        configure_file_logging(args.log_level)
        add_console_logging(args.log_level) # ADDED THIS CALL
        logging.info(f"--- Data Preprocessing Logging Initialized (Level: {args.log_level.upper()}) ---")
    except Exception as e:
        # Use basic print if logging setup itself fails
        print(f"ERROR setting up logging: {e}", file=sys.stderr)
        # Fallback basic config to console if setup fails
        logging.basicConfig(
            level=args.log_level.upper(), format="%(asctime)s - %(levelname)s - %(message)s"
        )
        logging.error(f"Failed to configure logging via utility: {e}", exc_info=True)
        # Decide if you want to exit if logging fails
        # sys.exit(1)

    # --- Define Standardized Filenames ---
    raw_filename = "raw_data.csv"
    raw_filepath = os.path.join(DATA_DIR, raw_filename)
    processed_filename = "processed_data.csv"
    processed_filepath = os.path.join(DATA_DIR, processed_filename)

    # --- Load Raw Data ---
    try:
        # Read raw data assuming 'date' and 'tic' are columns (as saved by fetch_data)
        raw_df = pd.read_csv(raw_filepath)
        raw_df['date'] = pd.to_datetime(raw_df['date']) # Ensure date is datetime
        logging.info(f"Loaded raw data from {raw_filepath}. Shape: {raw_df.shape}")
        # Check if 'vix' column exists
        if 'vix' not in raw_df.columns:
             logging.warning("VIX column not found in raw_data.csv. Ensure fetch_data includes it.")
        else:
             logging.info("VIX column found in raw_data.csv.")
    except FileNotFoundError:
        logging.error(f"Raw data file not found: {raw_filepath}. Run the fetch script first.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading raw data: {e}.")
        sys.exit(1)

    # --- Determine and Filter by End Date ---
    # Use the latest date needed (Test End Date) by default for filtering raw data
    # before preprocessing, unless overridden by command line.
    end_date_to_use = args.train_end_date if args.train_end_date else settings.TEST_END_DATE
    if args.train_end_date:
        logging.info(f"Using command-line override end date for filtering: {end_date_to_use}")
    else:
        logging.info(f"Using latest required end date from settings (TEST_END_DATE) for filtering: {end_date_to_use}")

    try:
        # Convert end_date_to_use to Timestamp for comparison (handle potential errors)
        end_date_ts = pd.Timestamp(end_date_to_use)
        filtered_df = raw_df[raw_df['date'] <= end_date_ts].copy()
        logging.info(f"Filtered data up to {end_date_to_use}. Shape after filtering: {filtered_df.shape}")
        if filtered_df.empty:
            logging.warning(f"Dataframe is empty after filtering by end date {end_date_to_use}. Check date range and data.")
    except ValueError:
         logging.error(f"Invalid date format provided for --train-end-date: '{args.train_end_date}'. Use YYYY-MM-DD.")
         sys.exit(1)
    except KeyError:
        logging.error(f"Column 'date' not found for filtering. Columns: {raw_df.columns.tolist()}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error filtering DataFrame by date: {e}")
        sys.exit(1)


    # --- Preprocess Filtered Data ---
    if not filtered_df.empty:
        # Set 'date' as index before preprocessing if it's not already
        if not isinstance(filtered_df.index, pd.DatetimeIndex):
            try:
                filtered_df = filtered_df.set_index('date')
            except KeyError:
                 logging.error("Could not set 'date' as index before preprocessing.")
                 sys.exit(1)

        processed_df = preprocess_data(filtered_df, is_live_trading=False) # Always False when run as script

        if processed_df is not None and not processed_df.empty:
            # --- Save Processed Data ---
            try:
                # preprocess_data now returns df with 'date' as a column, index is numerical
                processed_df.to_csv(processed_filepath, index=False)
                logging.info(f"Processed data saved to {processed_filepath}")
                logging.info(f"Final processed data range: {processed_df['date'].min()} to {processed_df['date'].max()}")
            except Exception as e:
                logging.error(f"Failed to save processed data: {e}")
        else:
            logging.error("Preprocessing returned None or an empty DataFrame.")
    else:
        logging.warning("Skipping preprocessing and saving as the filtered DataFrame was empty.")


    logging.info("--- Data Preprocessing Script Finished ---")
