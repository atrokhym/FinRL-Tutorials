# alpaca_trading_agent/src/preprocessing/preprocess_data.py

import pandas as pd
import numpy as np
from stockstats import StockDataFrame as Sdf
import os
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration Loading ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')

# Add config directory to sys.path
import sys
sys.path.insert(0, CONFIG_DIR)

try:
    import settings
except ImportError as e:
    logging.error(f"Error importing configuration: {e}. Ensure config/settings.py exists.")
    sys.exit(1)

# --- Preprocessing Function ---
def preprocess_data(df):
    """
    Preprocesses the raw stock data fetched from Alpaca.
    - Calculates technical indicators using stockstats.
    - Handles missing values.
     - Formats the DataFrame for FinRL environment.
     - NOTE: When using for live/incremental data, ensure the input `df`
       contains sufficient historical lookback rows for indicators to be calculated correctly.

     Args:
         df (pd.DataFrame): Raw data with columns like ['open', 'high', 'low', 'close', 'volume', 'tic'].
                            Index should be datetime. It should contain enough historical rows for indicator calculation.
         is_live_trading (bool, optional): If True, selects only the last row for prediction. Defaults to False.
 
     Returns:
         pd.DataFrame | None: Processed DataFrame ready for FinRL, or None if error.
     """
def preprocess_data(df: pd.DataFrame, is_live_trading: bool = False) -> pd.DataFrame | None:
    logging.info(f"Starting preprocessing. Initial shape: {df.shape}")

    # Convert index to datetime if it's not already (it should be from fetch_data)
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index)

    # Use StockDataFrame for indicator calculation
    # Ensure columns are named as expected by stockstats: open, high, low, close, volume
    # Our fetch_data script already lowercased them.
    stock = Sdf.retype(df.copy())

    # Use indicators defined in settings
    # State features: close, high, low, trade_count, open, volume, vwap
    # Technical indicators: macd, rsi_14, cci_14, boll_ub, boll_lb
    indicators = settings.INDICATORS

    # Filter out price/volume features that are already in the data
    tech_indicators = [ind for ind in indicators if ind not in ['close', 'high', 'low', 'trade_count', 'open', 'volume', 'vwap']]
    logging.info(f"Calculating indicators: {indicators}")

    for indicator in indicators:
        try:
            indicator_data = stock[indicator]
            df[indicator] = indicator_data
            logging.debug(f"Calculated {indicator}")
        except Exception as e:
            logging.warning(f"Could not calculate indicator '{indicator}': {e}")

    # Handle potential missing values generated by indicators (e.g., at the beginning)
    # Option 1: Forward fill
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., if ffill couldn't fill from the start) with 0
    df.fillna(0, inplace=True)
    # Option 2: Backward fill (less common for time series)
    # df.bfill(inplace=True)
    # Option 3: Fill with 0 or mean (use cautiously)
    # df.fillna(0, inplace=True)
    # Option 4: Drop rows with any NaN (might lose valuable data)
    # df.dropna(inplace=True) # Removed: Don't drop all rows with NaNs, we need the last row.

    # Ensure correct column naming and order for FinRL (often expects 'date' column)
    # df = df.reset_index().rename(columns={'index': 'date'}) # Removed: 'date' column should already exist from fetch_latest_data

    # --- Ensure 'date' column exists by resetting index ---
    # This needs to happen regardless of live trading or not, before sorting.
    original_index_name = df.index.name # Store original index name if it exists
    df = df.reset_index() # Reset index, making it a column
    # Rename the index column to 'date'. Common names are 'timestamp' or None (becomes 'index')
    index_col_name = original_index_name if original_index_name else 'index' # Default name is 'index' if unnamed
    if index_col_name in df.columns and 'date' not in df.columns:
        df = df.rename(columns={index_col_name: 'date'})
        logging.info(f"Renamed index column '{index_col_name}' to 'date'.")
    elif 'date' not in df.columns:
         # If rename didn't happen and 'date' still missing, log error
         logging.error(f"Could not find or create 'date' column from index '{index_col_name}'. Columns: {df.columns.tolist()}")
         return None

    # --- Select only the last row IF in live trading mode ---
    if is_live_trading:
        logging.info("Preprocessing for live trading: Selecting last row.")
        if not df.empty:
            df = df.iloc[[-1]].reset_index(drop=True) # Select last row and reset its new index
        else:
            # This case might be less likely now, but keep for safety
            logging.warning("DataFrame became empty before selecting last row.")
            return None
    else:
         logging.info("Preprocessing for training/backtesting: Using full timeseries.")

    # Ensure 'tic' column exists
    if 'tic' not in df.columns:
        logging.error("Preprocessing error: 'tic' column missing.")
        return None

    # Sort by date and ticker - crucial for FinRL environments
    if 'date' in df.columns and 'tic' in df.columns:
        df = df.sort_values(by=['date', 'tic']).reset_index(drop=True)
    else:
        logging.error(f"Required columns ('date', 'tic') not found for sorting. Columns: {df.columns.tolist()}")
        return None

    logging.info(f"Preprocessing finished. Final shape: {df.shape}")
    logging.info(f"Columns: {df.columns.tolist()}")
    logging.info(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # Check for any remaining NaN values
    if df.isnull().values.any():
        logging.warning("NaN values remain after preprocessing. Review handling.")
        logging.warning(df[df.isnull().any(axis=1)])

    return df

# --- Main Execution ---
if __name__ == "__main__":
    logging.info("--- Starting Data Preprocessing Script ---")

    # --- Load Raw Data ---
    train_filename = f"train_data_{settings.TRAIN_START_DATE}_{settings.TRAIN_END_DATE}.csv"
    train_filepath = os.path.join(DATA_DIR, train_filename)
    test_filename = f"test_data_{settings.TEST_START_DATE}_{settings.TEST_END_DATE}.csv"
    test_filepath = os.path.join(DATA_DIR, test_filename)

    try:
        raw_train_df = pd.read_csv(train_filepath, index_col=0, parse_dates=True)
        logging.info(f"Loaded raw training data from {train_filepath}")
    except FileNotFoundError:
        logging.error(f"Raw training data file not found: {train_filepath}")
        logging.error("Please run the data fetching script first.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading raw training data: {e}")
        sys.exit(1)

    try:
        raw_test_df = pd.read_csv(test_filepath, index_col=0, parse_dates=True)
        logging.info(f"Loaded raw testing data from {test_filepath}")
    except FileNotFoundError:
        logging.error(f"Raw testing data file not found: {test_filepath}")
        # Allow continuing if only training data exists, but warn
        logging.warning("Raw testing data file not found. Proceeding with training data only.")
        raw_test_df = None
    except Exception as e:
        logging.error(f"Error loading raw testing data: {e}")
        raw_test_df = None # Treat as non-critical for now

    # --- Preprocess Data ---
    processed_train_df = preprocess_data(raw_train_df)
    if processed_train_df is not None:
        # Save processed training data
        processed_train_filename = f"train_processed_{settings.TRAIN_START_DATE}_{settings.TRAIN_END_DATE}.csv"
        processed_train_filepath = os.path.join(DATA_DIR, processed_train_filename)
        try:
            processed_train_df.to_csv(processed_train_filepath, index=False)
            logging.info(f"Processed training data saved to {processed_train_filepath}")
        except Exception as e:
            logging.error(f"Failed to save processed training data: {e}")

    if raw_test_df is not None:
        processed_test_df = preprocess_data(raw_test_df)
        if processed_test_df is not None:
            # Save processed testing data
            processed_test_filename = f"test_processed_{settings.TEST_START_DATE}_{settings.TEST_END_DATE}.csv"
            processed_test_filepath = os.path.join(DATA_DIR, processed_test_filename)
            try:
                processed_test_df.to_csv(processed_test_filepath, index=False)
                logging.info(f"Processed testing data saved to {processed_test_filepath}")
            except Exception as e:
                logging.error(f"Failed to save processed testing data: {e}")

    logging.info("--- Data Preprocessing Script Finished ---")
