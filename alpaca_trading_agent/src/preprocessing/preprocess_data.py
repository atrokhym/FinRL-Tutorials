# alpaca_trading_agent/src/preprocessing/preprocess_data.py

import pandas as pd
import numpy as np
from stockstats import StockDataFrame as Sdf
import os
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration Loading ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')

# Add config directory to sys.path
import sys
sys.path.insert(0, CONFIG_DIR)

try:
    import settings
except ImportError as e:
    logging.error(f"Error importing configuration: {e}. Ensure config/settings.py exists.")
    sys.exit(1)

# --- Preprocessing Function ---
def preprocess_data(df: pd.DataFrame, is_live_trading: bool = False) -> pd.DataFrame | None:
    """
    Preprocesses the raw stock data fetched from Alpaca.
    - Calculates technical indicators using stockstats.
    - Calculates turbulence index (rolling std dev of returns).
    - Handles missing values.
    - Formats the DataFrame for FinRL environment.
    - NOTE: When using for live/incremental data, ensure the input `df`
      contains sufficient historical lookback rows for indicators to be calculated correctly.

     Args:
         df (pd.DataFrame): Raw data with columns like ['open', 'high', 'low', 'close', 'volume', 'tic'].
                            Index should be datetime. It should contain enough historical rows for indicator calculation.
         is_live_trading (bool, optional): If True, selects only the last row for prediction. Defaults to False.

     Returns:
         pd.DataFrame | None: Processed DataFrame ready for FinRL, or None if error.
     """
    # Define turbulence window
    TURBULENCE_WINDOW = 30

    logging.info(f"Starting preprocessing. Initial shape: {df.shape}")

    # Convert index to datetime if it's not already (it should be from fetch_data)
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index)

    # Use StockDataFrame for indicator calculation
    # Ensure columns are named as expected by stockstats: open, high, low, close, volume
    # Our fetch_data script already lowercased them.
    stock = Sdf.retype(df.copy())

    # Use indicators defined in settings
    # State features: close, high, low, trade_count, open, volume, vwap
    # Technical indicators: macd, rsi_14, cci_14, boll_ub, boll_lb
    indicators = settings.INDICATORS

    # Filter out price/volume features that are already in the data
    tech_indicators = [ind for ind in indicators if ind not in ['close', 'high', 'low', 'trade_count', 'open', 'volume', 'vwap']]
    logging.info(f"Calculating indicators: {indicators}")

    for indicator in indicators:
        try:
            indicator_data = stock[indicator]
            df[indicator] = indicator_data
            logging.debug(f"Calculated {indicator}")
        except Exception as e:
            logging.warning(f"Could not calculate indicator '{indicator}': {e}")

    # Handle potential missing values generated by indicators
    # Option 1: Forward fill (Handles NaNs from indicators at start)
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., if ffill couldn't fill from the start) with 0
    df.fillna(0, inplace=True)
    # Option 2: Backward fill (less common for time series)
    # df.bfill(inplace=True)
    # Option 3: Fill with 0 or mean (use cautiously)
    # df.fillna(0, inplace=True)
    # Option 4: Drop rows with any NaN (might lose valuable data)
    # df.dropna(inplace=True) # Removed: Don't drop all rows with NaNs, we need the last row.

    # Ensure correct column naming and order for FinRL (often expects 'date' column)
    # df = df.reset_index().rename(columns={'index': 'date'}) # Removed: 'date' column should already exist from fetch_latest_data

    # --- Ensure 'date' column exists by resetting index ---
    # This needs to happen regardless of live trading or not, before sorting.
    original_index_name = df.index.name # Store original index name if it exists
    df = df.reset_index() # Reset index, making it a column
    # Rename the index column to 'date'. Common names are 'timestamp' or None (becomes 'index')
    index_col_name = original_index_name if original_index_name else 'index' # Default name is 'index' if unnamed
    if index_col_name in df.columns and 'date' not in df.columns:
        df = df.rename(columns={index_col_name: 'date'})
        logging.info(f"Renamed index column '{index_col_name}' to 'date'.")
    elif 'date' not in df.columns:
         # If rename didn't happen and 'date' still missing, log error
         logging.error(f"Could not find or create 'date' column from index '{index_col_name}'. Columns: {df.columns.tolist()}")
         return None

    # --- Calculate Turbulence Index (After 'date' column exists) ---
    try:
        # Calculate daily returns per tic (requires 'date' and 'tic' columns)
        df = df.sort_values(by=['tic', 'date']) # Sort for pct_change
        df['daily_return'] = df.groupby('tic')['close'].pct_change()

        # Calculate rolling standard deviation of returns per tic
        df['turbulence'] = df.groupby('tic')['daily_return'].transform(lambda x: x.rolling(window=TURBULENCE_WINDOW, min_periods=TURBULENCE_WINDOW).std())

        # Drop the intermediate daily_return column
        df.drop(columns=['daily_return'], inplace=True)
        logging.info(f"Calculated turbulence index (rolling {TURBULENCE_WINDOW}-day std dev).")
    except KeyError as e:
         logging.error(f"KeyError during turbulence calculation (likely missing 'date' or 'tic'): {e}")
         return None
    except Exception as e:
         logging.error(f"Error during turbulence calculation: {e}", exc_info=True)
         return None
    # --- End Turbulence Calculation ---


    # --- Handle NaNs from Turbulence and Fill Remaining ---
    # Forward fill again to propagate last valid turbulence value if needed
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., at the very start of the series for indicators/turbulence) with 0
    df.fillna(0, inplace=True)


    # --- Select only the last row IF in live trading mode ---
    if is_live_trading:
        logging.info("Preprocessing for live trading: Selecting last row.")
        if not df.empty:
            df = df.iloc[[-1]].reset_index(drop=True) # Select last row and reset its new index
        else:
            # This case might be less likely now, but keep for safety
            logging.warning("DataFrame became empty before selecting last row.")
            return None
    else:
         logging.info("Preprocessing for training/backtesting: Using full timeseries.")

    # Ensure 'tic' column exists
    if 'tic' not in df.columns:
        logging.error("Preprocessing error: 'tic' column missing.")
        return None

    # Sort by date and ticker - crucial for FinRL environments
    if 'date' in df.columns and 'tic' in df.columns:
        df = df.sort_values(by=['date', 'tic']).reset_index(drop=True)
    else:
        logging.error(f"Required columns ('date', 'tic') not found for sorting. Columns: {df.columns.tolist()}")
        return None

    logging.info(f"Preprocessing finished. Final shape: {df.shape}")
    logging.info(f"Columns: {df.columns.tolist()}")
    logging.info(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # Check for any remaining NaN values
    if df.isnull().values.any():
        logging.warning("NaN values remain after preprocessing. Review handling.")
        logging.warning(df[df.isnull().any(axis=1)])

    return df

# --- Main Execution ---
if __name__ == "__main__":
    logging.info("--- Starting Data Preprocessing Script (for Walk-Forward) ---")

    # --- Load Raw Data (Combine Train and Test for Full Period) ---
    train_filename = f"train_data_{settings.TRAIN_START_DATE}_{settings.TRAIN_END_DATE}.csv"
    train_filepath = os.path.join(DATA_DIR, train_filename)
    test_filename = f"test_data_{settings.TEST_START_DATE}_{settings.TEST_END_DATE}.csv"
    test_filepath = os.path.join(DATA_DIR, test_filename)

    loaded_dfs = []
    try:
        raw_train_df = pd.read_csv(train_filepath, index_col=0, parse_dates=True)
        loaded_dfs.append(raw_train_df)
        logging.info(f"Loaded raw training data from {train_filepath} ({raw_train_df.index.min().date()} to {raw_train_df.index.max().date()})")
    except FileNotFoundError:
        logging.error(f"Raw training data file not found: {train_filepath}. Cannot proceed.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading raw training data: {e}. Cannot proceed.")
        sys.exit(1)

    try:
        raw_test_df = pd.read_csv(test_filepath, index_col=0, parse_dates=True)
        loaded_dfs.append(raw_test_df)
        logging.info(f"Loaded raw testing data from {test_filepath} ({raw_test_df.index.min().date()} to {raw_test_df.index.max().date()})")
    except FileNotFoundError:
        logging.warning(f"Raw testing data file not found: {test_filepath}. Combining training data only.")
    except Exception as e:
        logging.error(f"Error loading raw testing data: {e}. Combining training data only.")

    if not loaded_dfs:
        logging.error("No data loaded. Exiting.")
        sys.exit(1)

    # Combine DataFrames
    full_raw_df = pd.concat(loaded_dfs).sort_index()
    logging.info(f"Combined raw data. Shape: {full_raw_df.shape}, Date range: {full_raw_df.index.min().date()} to {full_raw_df.index.max().date()}")

    # --- Preprocess Combined Data ---
    processed_full_df = preprocess_data(full_raw_df)

    if processed_full_df is not None:
        # Save the single combined processed file
        processed_full_filename = "full_processed_combined.csv" # Simple filename
        processed_full_filepath = os.path.join(DATA_DIR, processed_full_filename)
        try:
            processed_full_df.to_csv(processed_full_filepath, index=False)
            logging.info(f"Full processed data saved to {processed_full_filepath}")
        except Exception as e:
            logging.error(f"Failed to save full processed data: {e}")
    else:
        logging.error("Preprocessing failed for the combined dataset.")

    logging.info("--- Data Preprocessing Script Finished ---")
