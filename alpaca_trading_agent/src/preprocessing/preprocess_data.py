# alpaca_trading_agent/src/preprocessing/preprocess_data.py

import pandas as pd
import numpy as np
from stockstats import StockDataFrame as Sdf
import os
import logging
import argparse # Added argparse
import sys # Added sys

# Add src directory to sys.path to allow importing utils
SRC_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, SRC_DIR)

# Logging setup will be done explicitly using the shared utility
from utils.logging_setup import configure_file_logging
# --- Configuration Loading ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')
DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
# SRC_DIR is already defined above

# Add config directory to sys.path
# import sys # Already imported above
sys.path.insert(0, CONFIG_DIR)

try:
    import settings
except ImportError as e:
    logging.error(f"Error importing configuration: {e}. Ensure config/settings.py exists.")
    sys.exit(1)

# --- Preprocessing Function ---
def preprocess_data(df: pd.DataFrame, is_live_trading: bool = False) -> pd.DataFrame | None:
    """
    Preprocesses the raw stock data fetched from Alpaca.
    - Calculates technical indicators using stockstats.
    - Calculates turbulence index (rolling std dev of returns).
    - Handles missing values.
    - Formats the DataFrame for FinRL environment.
    - NOTE: When using for live/incremental data, ensure the input `df`
      contains sufficient historical lookback rows for indicators to be calculated correctly.

     Args:
         df (pd.DataFrame): Raw data with columns like ['open', 'high', 'low', 'close', 'volume', 'tic'].
                            Index should be datetime. It should contain enough historical rows for indicator calculation.
         is_live_trading (bool, optional): If True, selects only the last row for prediction. Defaults to False.

     Returns:
         pd.DataFrame | None: Processed DataFrame ready for FinRL, or None if error.
     """
    # Define turbulence window
    TURBULENCE_WINDOW = 30

    logging.info(f"Starting preprocessing. Initial shape: {df.shape}")

    # Convert index to datetime if it's not already (it should be from fetch_data)
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index)

    # Use StockDataFrame for indicator calculation
    # Ensure columns are named as expected by stockstats: open, high, low, close, volume
    # Our fetch_data script already lowercased them.
    stock = Sdf.retype(df.copy())

    # Use indicators defined in settings
    # State features: close, high, low, trade_count, open, volume, vwap
    # Technical indicators: macd, rsi_14, cci_14, boll_ub, boll_lb
    indicators = settings.INDICATORS

    # Filter out price/volume features that are already in the data
    tech_indicators = [ind for ind in indicators if ind not in ['close', 'high', 'low', 'trade_count', 'open', 'volume', 'vwap']]
    logging.info(f"Calculating indicators: {indicators}")

    for indicator in indicators:
        try:
            indicator_data = stock[indicator]
            df[indicator] = indicator_data
            logging.debug(f"Calculated {indicator}")
        except Exception as e:
            logging.warning(f"Could not calculate indicator '{indicator}': {e}")

    # Handle potential missing values generated by indicators
    # Option 1: Forward fill (Handles NaNs from indicators at start)
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., if ffill couldn't fill from the start) with 0
    df.fillna(0, inplace=True)
    # Option 2: Backward fill (less common for time series)
    # df.bfill(inplace=True)
    # Option 3: Fill with 0 or mean (use cautiously)
    # df.fillna(0, inplace=True)
    # Option 4: Drop rows with any NaN (might lose valuable data)
    # df.dropna(inplace=True) # Removed: Don't drop all rows with NaNs, we need the last row.

    # Ensure correct column naming and order for FinRL (often expects 'date' column)
    # df = df.reset_index().rename(columns={'index': 'date'}) # Removed: 'date' column should already exist from fetch_latest_data

    # --- Ensure 'date' column exists by resetting index ---
    # This needs to happen regardless of live trading or not, before sorting.
    original_index_name = df.index.name # Store original index name if it exists
    df = df.reset_index() # Reset index, making it a column
    # Rename the index column to 'date'. Common names are 'timestamp' or None (becomes 'index')
    index_col_name = original_index_name if original_index_name else 'index' # Default name is 'index' if unnamed
    if index_col_name in df.columns and 'date' not in df.columns:
        df = df.rename(columns={index_col_name: 'date'})
        logging.info(f"Renamed index column '{index_col_name}' to 'date'.")
    elif 'date' not in df.columns:
         # If rename didn't happen and 'date' still missing, log error
         logging.error(f"Could not find or create 'date' column from index '{index_col_name}'. Columns: {df.columns.tolist()}")
         return None

    # --- Calculate Turbulence Index (After 'date' column exists) ---
    try:
        # Calculate daily returns per tic (requires 'date' and 'tic' columns)
        df = df.sort_values(by=['tic', 'date']) # Sort for pct_change
        df['daily_return'] = df.groupby('tic')['close'].pct_change()

        # Calculate rolling standard deviation of returns per tic
        df['turbulence'] = df.groupby('tic')['daily_return'].transform(lambda x: x.rolling(window=TURBULENCE_WINDOW, min_periods=TURBULENCE_WINDOW).std())

        # Drop the intermediate daily_return column
        df.drop(columns=['daily_return'], inplace=True)
        logging.info(f"Calculated turbulence index (rolling {TURBULENCE_WINDOW}-day std dev).")
    except KeyError as e:
         logging.error(f"KeyError during turbulence calculation (likely missing 'date' or 'tic'): {e}")
         return None
    except Exception as e:
         logging.error(f"Error during turbulence calculation: {e}", exc_info=True)
         return None
    # --- End Turbulence Calculation ---


    # --- Handle NaNs from Turbulence and Fill Remaining ---
    # Forward fill again to propagate last valid turbulence value if needed
    df.ffill(inplace=True)
    # Fill any remaining NaNs (e.g., at the very start of the series for indicators/turbulence) with 0
    df.fillna(0, inplace=True)


    # --- Select only the last row IF in live trading mode ---
    if is_live_trading:
        logging.info("Preprocessing for live trading: Selecting last row.")
        if not df.empty:
            df = df.iloc[[-1]].reset_index(drop=True) # Select last row and reset its new index
        else:
            # This case might be less likely now, but keep for safety
            logging.warning("DataFrame became empty before selecting last row.")
            return None
    else:
         logging.info("Preprocessing for training/backtesting: Using full timeseries.")

    # Ensure 'tic' column exists
    if 'tic' not in df.columns:
        logging.error("Preprocessing error: 'tic' column missing.")
        return None

    # Sort by date and ticker - crucial for FinRL environments
    if 'date' in df.columns and 'tic' in df.columns:
        df = df.sort_values(by=['date', 'tic']).reset_index(drop=True)
    else:
        logging.error(f"Required columns ('date', 'tic') not found for sorting. Columns: {df.columns.tolist()}")
        return None

    logging.info(f"Preprocessing finished. Final shape: {df.shape}")
    logging.info(f"Columns: {df.columns.tolist()}")
    logging.info(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # Check for any remaining NaN values
    if df.isnull().values.any():
        logging.warning("NaN values remain after preprocessing. Review handling.")
        logging.warning(df[df.isnull().any(axis=1)])

    return df

# Logging setup is handled by the calling script (main.py)
# This script inherits the logger configuration.
# --- Main Execution ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data Preprocessor for Alpaca Trading Agent")
    parser.add_argument(
        '--log-level',
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help="Set the logging level for the script."
    )
    parser.add_argument(
        '--train-end-date',
        type=str,
        default=None,
        help="Specify the end date for filtering data in YYYY-MM-DD format. Overrides settings.py TRAIN_END_DATE."
    )
    args = parser.parse_args()

    # --- Configure Logging for this script ---
    configure_file_logging(args.log_level)
    # Note: Console logging is not added here by default.

    logging.info(f"--- Starting Data Preprocessing Script (PID: {os.getpid()}) ---")

    # --- Define Standardized Filenames ---
    raw_filename = "raw_data.csv"
    raw_filepath = os.path.join(DATA_DIR, raw_filename)
    processed_filename = "processed_data.csv"
    processed_filepath = os.path.join(DATA_DIR, processed_filename)

    # --- Load Raw Data ---
    try:
        raw_df = pd.read_csv(raw_filepath, index_col=0, delimiter='|')  # Specify pipe delimiter
        # Ensure index is datetime for filtering
        raw_df.index = pd.to_datetime(raw_df.index, format='%Y-%m-%d')  # Specify date format explicitly
        logging.info(f"Loaded raw data from {raw_filepath}. Shape: {raw_df.shape}")
    except FileNotFoundError:
        logging.error(f"Raw data file not found: {raw_filepath}. Run the fetch script first.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading raw data: {e}.")
        sys.exit(1)

    # --- Determine and Filter by End Date ---
    end_date_to_use = args.train_end_date if args.train_end_date else settings.TRAIN_END_DATE
    if args.train_end_date:
        logging.info(f"Using command-line end date for filtering: {end_date_to_use}")
    else:
        logging.info(f"Using end date from settings for filtering: {end_date_to_use}")

    try:
        # Convert end_date_to_use to Timestamp for comparison (handle potential errors)
        end_date_ts = pd.Timestamp(end_date_to_use)
        filtered_df = raw_df[raw_df.index <= end_date_ts].copy()
        logging.info(f"Filtered data up to {end_date_to_use}. Shape after filtering: {filtered_df.shape}")
        if filtered_df.empty:
            logging.warning(f"Dataframe is empty after filtering by end date {end_date_to_use}. Check date range and data.")
    except ValueError:
         logging.error(f"Invalid date format provided for --train-end-date: '{args.train_end_date}'. Use YYYY-MM-DD.")
         sys.exit(1)
    except Exception as e:
        logging.error(f"Error filtering DataFrame by date: {e}")
        sys.exit(1)


    # --- Preprocess Filtered Data ---
    if not filtered_df.empty:
        processed_df = preprocess_data(filtered_df, is_live_trading=False) # Always False when run as script

        if processed_df is not None and not processed_df.empty:
            # --- Save Processed Data ---
            try:
                processed_df.to_csv(processed_filepath, index=False) # index=False as preprocess_data adds 'date' column
                logging.info(f"Processed data saved to {processed_filepath}")
                logging.info(f"Final processed data range: {processed_df['date'].min()} to {processed_df['date'].max()}")
            except Exception as e:
                logging.error(f"Failed to save processed data: {e}")
        else:
            logging.error("Preprocessing returned None or an empty DataFrame.")
    else:
        logging.warning("Skipping preprocessing and saving as the filtered DataFrame was empty.")


    logging.info("--- Data Preprocessing Script Finished ---")
