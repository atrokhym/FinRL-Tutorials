{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPkl_Mvu9fF5"
   },
   "source": [
    "# Crypto Trading Feature Importance Analysis for Deep Reinforcement Learning\n",
    "\n",
    "Notebook based on:\n",
    "\n",
    "0: Lopez de Prado, M. (2018). Advances in financial machine learning. John Wiley & Sons.\n",
    "\n",
    "https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089\n",
    "\n",
    "1: AI4Finance Foundation\n",
    "\n",
    "https://github.com/AI4Finance-Foundation\n",
    "\n",
    "2: Optimal Trading Rules Detection with Triple Barrier Labeling\n",
    "\n",
    "https://www.youtube.com/watch?v=U2CxilKFue4\n",
    "\n",
    "3: Data Labelling, the Triple-barrier Method\n",
    "\n",
    "https://towardsdatascience.com/the-triple-barrier-method-251268419dcd\n",
    "\n",
    "\n",
    "4: Financial Machine Learning Part 1: Labels\n",
    "\n",
    "https://towardsdatascience.com/financial-machine-learning-part-1-labels-7eeed050f32e#:~:text=Adding%20Path%20Dependency%3A%20Triple%2DBarrier,%3A%20the%20triple%2Dbarrier%20method.\n",
    "\n",
    "\n",
    "5: Meta-Labeling: Solving for Non Stationarity and Position Sizing\n",
    "\n",
    "https://www.youtube.com/watch?v=WbgglcXfEzA\n",
    "\n",
    "\n",
    "6: Advances in Financial Machine Learning\n",
    "\n",
    "https://github.com/JackBrady/Financial-Machine-Learning/blob/master/USDJPY_Notebook.ipynb\n",
    "\n",
    "\n",
    "Specifically, important features for currently one coin at the time only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93NXSRXxk5oh",
    "outputId": "ba319bea-aefa-4a5f-9a9a-b31e807767f8"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "%cd /tmp\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!git clone https://github.com/AI4Finance-Foundation/FinRL-Meta\n",
    "%cd /tmp/FinRL-Meta/\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/ElegantRL.git\n",
    "# !pip install -q condacolab\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "!pip install gputil\n",
    "!pip install trading_calendars\n",
    "!pip install python-binance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCumXUTo22tu",
    "outputId": "f5777aa9-db9f-4995-ce5a-70ab89b8101e"
   },
   "outputs": [],
   "source": [
    "#install TA-lib (technical analysis)\n",
    "# !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz \n",
    "# !tar xvzf ta-lib-0.4.0-src.tar.gz\n",
    "# import os\n",
    "# os.chdir('ta-lib') \n",
    "# !./configure --prefix=/usr\n",
    "# !make\n",
    "# !make install\n",
    "# os.chdir('../')\n",
    "# !cd\n",
    "# !pip install TA-Lib\n",
    "!pip install ta-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbql-67vW_cv"
   },
   "outputs": [],
   "source": [
    "# Other imports\n",
    "\n",
    "import scipy as sp\n",
    "import math\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from meta.data_processor import DataProcessor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from talib.abstract import MACD, RSI, CCI, DX\n",
    "from binance.client import Client\n",
    "from pandas.testing import assert_frame_equal\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGYjgA5KVciR"
   },
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "\n",
    "SCALE_FACTOR = 2\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [5 * SCALE_FACTOR, 2 * SCALE_FACTOR]\n",
    "plt.rcParams['figure.dpi'] = 300 * SCALE_FACTOR\n",
    "plt.rcParams['font.size'] = 5 * SCALE_FACTOR\n",
    "plt.rcParams['axes.labelsize'] = 5 * SCALE_FACTOR\n",
    "plt.rcParams['axes.titlesize'] = 6 * SCALE_FACTOR\n",
    "plt.rcParams['xtick.labelsize'] = 4 * SCALE_FACTOR\n",
    "plt.rcParams['ytick.labelsize'] = 4 * SCALE_FACTOR\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4EJ9VW3mvQl"
   },
   "source": [
    "# Part 1: Adapted Binance downloader \n",
    "\n",
    "\n",
    "Any features you think of are probably coming out of OHLCV data or alternative data streams. The functions required to obtain the new features are added here and added to the eventual dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn6lsnoimkX5"
   },
   "source": [
    "## 1.1 Set contants and use BinanceProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2rOJoF5nDY5"
   },
   "outputs": [],
   "source": [
    "# Set constants:\n",
    "\n",
    "ticker_list = ['ETHUSDT']\n",
    "\n",
    "\n",
    "TIME_INTERVAL = '1h'\n",
    "\n",
    "TRAIN_START_DATE = '2015-01-01'\n",
    "TRAIN_END_DATE= '2019-08-01'\n",
    "TRADE_START_DATE = '2019-08-01'\n",
    "TRADE_END_DATE = '2020-01-03'\n",
    "\n",
    "\n",
    "technical_indicator_list = ['macd',\n",
    "                             'macd_signal',\n",
    "                             'macd_hist',\n",
    "                             'rsi',\n",
    "                             'cci',\n",
    "                             'dx'\n",
    "                             ]\n",
    "\n",
    "if_vix = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWvm7WQ0nCBr",
    "outputId": "39e3cd74-afe1-4e09-d19b-aaa21d7ab5e0"
   },
   "outputs": [],
   "source": [
    "# Process data using unified data processor\n",
    "p = DataProcessor(data_source='binance', start_date=TRAIN_START_DATE, end_date=TRADE_END_DATE, time_interval=TIME_INTERVAL)\n",
    "p.download_data(ticker_list=ticker_list)\n",
    "p.clean_data()\n",
    "df = p.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OWfalHj87aEp",
    "outputId": "1bbc1861-0f4d-4a47-b2d4-480135a01a74"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChdO8tqhvg93"
   },
   "source": [
    "## 1.2 Add technical indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc3FUtJfRvXA"
   },
   "outputs": [],
   "source": [
    "def add_technical_indicator(df, tech_indicator_list):\n",
    "    # print('Adding self-defined technical indicators is NOT supported yet.')\n",
    "    # print('Use default: MACD, RSI, CCI, DX.')\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in df.tic.unique():\n",
    "        tic_df = df[df.tic == i].copy()\n",
    "        tic_df['rsi'] = RSI(tic_df['close'], timeperiod=14)\n",
    "        tic_df['macd'], tic_df['macd_signal'], tic_df['macd_hist'] = MACD(tic_df['close'], fastperiod=12,\n",
    "                                                                          slowperiod=26, signalperiod=9)\n",
    "        tic_df['cci'] = CCI(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        tic_df['dx'] = DX(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        final_df = final_df._append(tic_df)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "nkI0v73_RvyR",
    "outputId": "19d2d266-e15f-4eef-8d25-f29217696f28"
   },
   "outputs": [],
   "source": [
    "processed_df=add_technical_indicator(df,technical_indicator_list)\n",
    "processed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicator(df, tech_indicator_list):\n",
    "    # print('Adding self-defined technical indicators is NOT supported yet.')\n",
    "    # print('Use default: MACD, RSI, CCI, DX.')\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in df.tic.unique():\n",
    "        tic_df = df[df.tic == i].copy()\n",
    "        tic_df['rsi'] = RSI(tic_df['close'], timeperiod=14)\n",
    "        tic_df['macd'], tic_df['macd_signal'], tic_df['macd_hist'] = MACD(tic_df['close'], fastperiod=12,\n",
    "                                                                          slowperiod=26, signalperiod=9)\n",
    "        tic_df['cci'] = CCI(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        tic_df['dx'] = DX(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        final_df = final_df.append(tic_df)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicator(df, tech_indicator_list):\n",
    "    # print('Adding self-defined technical indicators is NOT supported yet.')\n",
    "    # print('Use default: MACD, RSI, CCI, DX.')\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in df.tic.unique():\n",
    "        tic_df = df[df.tic == i].copy()\n",
    "        tic_df['rsi'] = RSI(tic_df['close'], timeperiod=14)\n",
    "        tic_df['macd'], tic_df['macd_signal'], tic_df['macd_hist'] = MACD(tic_df['close'], fastperiod=12,\n",
    "                                                                          slowperiod=26, signalperiod=9)\n",
    "        tic_df['cci'] = CCI(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        tic_df['dx'] = DX(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        final_df = final_df.append(tic_df)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicator(df, tech_indicator_list):\n",
    "    # print('Adding self-defined technical indicators is NOT supported yet.')\n",
    "    # print('Use default: MACD, RSI, CCI, DX.')\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in df.tic.unique():\n",
    "        tic_df = df[df.tic == i].copy()\n",
    "        tic_df['rsi'] = RSI(tic_df['close'], timeperiod=14)\n",
    "        tic_df['macd'], tic_df['macd_signal'], tic_df['macd_hist'] = MACD(tic_df['close'], fastperiod=12,\n",
    "                                                                          slowperiod=26, signalperiod=9)\n",
    "        tic_df['cci'] = CCI(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        tic_df['dx'] = DX(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
    "        final_df = final_df.append(tic_df)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofidEftSR4BC",
    "outputId": "9a7df539-ea90-47e5-a1d5-04dcc464a6ab"
   },
   "outputs": [],
   "source": [
    "# Drop unecessary columns and make time as index\n",
    "processed_df.index=pd.to_datetime(processed_df.time)\n",
    "processed_df.drop('time', inplace=True, axis=1)\n",
    "print(processed_df.tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59eD5HUo1axI"
   },
   "source": [
    "# Part 2: Triple barrier method/Data Labeling\n",
    "Introduction here:\n",
    "\n",
    "https://www.youtube.com/watch?v=U2CxilKFue4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Rvx0yzvt-B"
   },
   "source": [
    "## 2.1 Add volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2K6t6T51dxr"
   },
   "outputs": [],
   "source": [
    "def get_vol(prices, span=100):\n",
    "    # 1. compute returns of the form p[t]/p[t-1] - 1\n",
    "    df0 = prices.pct_change()\n",
    "    # 2. estimate rolling standard deviation\n",
    "    df0 = df0.ewm(span=span).std()\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRSOgoCcsHmU"
   },
   "outputs": [],
   "source": [
    "data_ohlcv = processed_df.assign(volatility=get_vol(processed_df.close)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "id": "g63UkMTCCkLS",
    "outputId": "7ab70fe5-9c3e-4169-fcb5-128b0227a1ec"
   },
   "outputs": [],
   "source": [
    "data_ohlcv.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGa9sSGd_GdK"
   },
   "source": [
    "## 2.2 Adding Path Dependency: Triple-Barrier Method\n",
    "\n",
    "To better incorporate the stop-loss and take-profit scenarios of a hypothetical trading strategy, we will modify the fixed-horizon labeling method so that it reflects which barrier has been touched first — upper, lower, or horizon. Hence the name: the triple-barrier method.\n",
    "\n",
    "The labeling schema is defined as follows:\n",
    "\n",
    "* y = 2 : top barrier is hit first\n",
    "* y = 1 : right barrier is hit first\n",
    "* y = 0 : bottom barrier is hit first\n",
    "\n",
    "What about the side of the bet?\n",
    "\n",
    "The schema above works fine for long-only strategies, however things get more complicated when we allow for both long and short bets. If we are betting short, our profit/loss is inverted relative to the price action — we profit if the price goes down and we lose when the price goes up.\n",
    "\n",
    "In order to account for this, we can simply represent side as 2 for long and 0 for short. Thus we can multiply our returns by the side, so whenever we’re betting short the negative returns become positive and vice-versa. Effectively, we flip the y = 2 and y = 0 labels if side = 0.\n",
    "\n",
    "Let’s take a shot at the implementation (based on MLDP’s code).\n",
    "First, we define the procedure for getting the timestamps of the horizon barriers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VoiqRnSCIJd"
   },
   "source": [
    "### Create function to obtain the barrier hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OUo9mkeCKny"
   },
   "outputs": [],
   "source": [
    "def get_barriers():\n",
    "  #create a container\n",
    "  barriers = pd.DataFrame(columns=['days_passed', \n",
    "            'price', 'vert_barrier', \\\n",
    "            'top_barrier', 'bottom_barrier'], \\\n",
    "              index = daily_volatility.index)\n",
    "  for day, vol in daily_volatility.items():\n",
    "    days_passed = len(daily_volatility.loc \\\n",
    "                  [daily_volatility.index[0] : day])\n",
    "    #set the vertical barrier \n",
    "    if (days_passed + t_final < len(daily_volatility.index) \\\n",
    "        and t_final != 0):\n",
    "        vert_barrier = daily_volatility.index[\n",
    "                            days_passed + t_final]\n",
    "    else:\n",
    "        vert_barrier = np.nan\n",
    "    #set the top barrier\n",
    "    if upper_lower_multipliers[0] > 0:\n",
    "        top_barrier = prices.loc[day] + prices.loc[day] * \\\n",
    "                      upper_lower_multipliers[0] * vol\n",
    "    else:\n",
    "        #set it to NaNs\n",
    "        top_barrier = pd.Series(index=prices.index)\n",
    "    #set the bottom barrier\n",
    "    if upper_lower_multipliers[1] > 0:\n",
    "        bottom_barrier = prices.loc[day] - prices.loc[day] * \\\n",
    "                      upper_lower_multipliers[1] * vol\n",
    "    else: \n",
    "        #set it to NaNs\n",
    "        bottom_barrier = pd.Series(index=prices.index)\n",
    "        \n",
    "    barriers.loc[day, ['days_passed', 'price', 'vert_barrier','top_barrier', 'bottom_barrier']] = \\\n",
    "    days_passed, prices.loc[day], vert_barrier, top_barrier, bottom_barrier\n",
    "\n",
    "  return barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gh0tSPaCnNZ"
   },
   "outputs": [],
   "source": [
    "# Set barrier parameters\n",
    "\n",
    "daily_volatility = data_ohlcv['volatility']\n",
    "t_final = 25\n",
    "upper_lower_multipliers = [2, 2]\n",
    "price = data_ohlcv['close']\n",
    "prices = price[daily_volatility.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "KBvziyM1FqAa",
    "outputId": "8c1bb7be-5b81-4752-dd51-390d6c7b8642"
   },
   "outputs": [],
   "source": [
    "barriers = get_barriers()\n",
    "barriers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FktzhO5gChKq"
   },
   "source": [
    "## 2.3 Function to get label for the dataset (0, 1, 2)\n",
    "\n",
    "* 0: hit the stoploss\n",
    "* 1: hit the time out\n",
    "* 2: hit the profit take\n",
    "\n",
    "The part in this function (commented out), allows for easy conversion to a regression analysis (currently it is classification). If one changes the labels to (-1, 0, 1), and change the hit on the vertical barrier to the function stated below.\n",
    "\n",
    "That will make hitting the profit take barrier 1, the vertical barrier a range from (-1, 1), and the stoploss barrier -1. This is a continuos space then.\n",
    "\n",
    "```\n",
    "barriers['out'][i] = max(\n",
    "          [(price_final - price_initial)/ \n",
    "            (top_barrier - price_initial), \\\n",
    "            (price_final - price_initial)/ \\\n",
    "            (price_initial - bottom_barrier)],\\\n",
    "            key=abs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHNWUpFPGAcO"
   },
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    \"\"\"\n",
    "    start: first day of the window\n",
    "    end:last day of the window\n",
    "    price_initial: first day stock price\n",
    "    price_final:last day stock price\n",
    "    top_barrier: profit taking limit\n",
    "    bottom_barrier:stop loss limt\n",
    "    condition_pt:top_barrier touching conditon\n",
    "    condition_sl:bottom_barrier touching conditon\n",
    "    \"\"\"\n",
    "    barriers[\"label_barrier\"] = None\n",
    "    for i in range(len(barriers.index)):\n",
    "        start = barriers.index[i]\n",
    "        end = barriers.vert_barrier[i]\n",
    "        if pd.notna(end):\n",
    "            # assign the initial and final price\n",
    "            # price_initial = barriers.price[start]\n",
    "            # price_final = barriers.price[end]\n",
    "            # assign the top and bottom barriers\n",
    "            top_val = barriers.top_barrier[i]\n",
    "            bottom_val = barriers.bottom_barrier[i]\n",
    "            top_barrier = top_val if np.isscalar(top_val) else (top_val.iloc[0] if not top_val.empty else np.nan)\n",
    "            bottom_barrier = bottom_val if np.isscalar(bottom_val) else (bottom_val.iloc[0] if not bottom_val.empty else np.nan)\n",
    "\n",
    "            # set the profit taking and stop loss conditons\n",
    "            idx = data_ohlcv.index.slice_indexer(start, end)\n",
    "            slice_close = data_ohlcv['close'].iloc[idx]\n",
    "            condition_pt = (slice_close >= top_barrier).any()\n",
    "            condition_sl = (slice_close <= bottom_barrier).any()\n",
    "\n",
    "            # assign the labels\n",
    "            if condition_pt:\n",
    "                barriers[\"label_barrier\"][i] = 2\n",
    "            elif condition_sl:\n",
    "                barriers[\"label_barrier\"][i] = 0\n",
    "            else:\n",
    "                # Change to regression analysis by switching labels (-1, 0, 1)\n",
    "                # and uncommenting the alternative function for vert barrier\n",
    "                barriers[\"label_barrier\"][i] = 1\n",
    "                # barriers['label_barrier'][i] = max(\n",
    "                #           [(price_final - price_initial)/\n",
    "                #             (top_barrier - price_initial), \\\n",
    "                #             (price_final - price_initial)/ \\\n",
    "                #             (price_initial - bottom_barrier)],\\\n",
    "                #             key=abs)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "aUq71tykusGG",
    "outputId": "59dba93f-73e4-46a7-f44f-a28c4cda6ccf"
   },
   "outputs": [],
   "source": [
    "# Use function to produce barriers\n",
    "\n",
    "get_labels()\n",
    "barriers\n",
    "\n",
    "# Merge the barriers with the main dataset and drop the last t_final + 1 barriers (as they are too close to the end)\n",
    "\n",
    "data_ohlcv = data_ohlcv.merge(barriers[['vert_barrier', 'top_barrier', 'bottom_barrier', 'label_barrier']], left_on='time', right_on='time')\n",
    "data_ohlcv.drop(data_ohlcv.tail(t_final + 1).index,inplace = True)\n",
    "data_ohlcv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiteMW0qGw8p",
    "outputId": "15df025f-857a-491d-bde6-55b201c273bb"
   },
   "outputs": [],
   "source": [
    "# Count barrier hits ( 0 = stoploss, 1 = timeout, 2 = profit take)\n",
    "pd.Series(data_ohlcv['label_barrier']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "4OUIDcJrHLkf",
    "outputId": "cac17e31-b19b-4791-96dc-95aa8ec8208c"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "\n",
    "\n",
    "TIMESTAMP_TO_PLOT= 300\n",
    "\n",
    "ax.set(title='ETH/USDT',\n",
    "       xlabel='date', ylabel='price')\n",
    "ax.plot(data_ohlcv.close[200:600])\n",
    "\n",
    "start = data_ohlcv.index[TIMESTAMP_TO_PLOT]\n",
    "end = data_ohlcv.vert_barrier[TIMESTAMP_TO_PLOT]\n",
    "upper_barrier = data_ohlcv.top_barrier[TIMESTAMP_TO_PLOT]\n",
    "lower_barrier = data_ohlcv.bottom_barrier[TIMESTAMP_TO_PLOT]\n",
    "\n",
    "ax.plot([start, end], [upper_barrier, upper_barrier], 'r--');\n",
    "ax.plot([start, end], [lower_barrier, lower_barrier], 'r--');\n",
    "ax.plot([start, end], [(lower_barrier + upper_barrier)*0.5, \\\n",
    "                       (lower_barrier + upper_barrier)*0.5], 'r--');\n",
    "ax.plot([start, start], [lower_barrier, upper_barrier], 'r-');\n",
    "ax.plot([end, end], [lower_barrier, upper_barrier], 'r-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdVqwKVKxetM"
   },
   "source": [
    "# Part 3: Copying the Neural Network present in ElegantRL ActorPPO agent.\n",
    "\n",
    "In ElegantRL from AI4Finance, all the preprogrammed Agents are present:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl/agents/net.py\n",
    "\n",
    "Some of the actions output discrete actions (classification), and some continuous actions (regression). This notebook can be adapted for both. by turning the labeling method in a (-1, 0, 1) and changing the Neural network to output a continuous space between -1 and 1.\n",
    "\n",
    "Therefore this notebook allows for analysis for both regression and classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiCj3qRwkfXF"
   },
   "outputs": [],
   "source": [
    "data_ohlcv = data_ohlcv.drop(['vert_barrier', 'top_barrier', 'bottom_barrier','adjusted_close','tic'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ImrJ30EwZo3",
    "outputId": "513d196f-67aa-42f2-d5d1-78fa0be252fb"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.state_dim = 12       # all the features\n",
    "        self.mid_dim = 2**10      # net dimension\n",
    "        self.action_dim = 3       # output (sell/nothing/buy)\n",
    "\n",
    "        # make a copy of the model in ActorPPO (activation function in forward function)\n",
    "\n",
    "        # Original initial layers\n",
    "        self.fc1 = nn.Linear(self.state_dim, self.mid_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.mid_dim, self.mid_dim)\n",
    "\n",
    "        # Original residual layers\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(self.mid_dim, self.mid_dim)\n",
    "        self.hw1 = nn.Hardswish()\n",
    "        self.fc_out = nn.Linear(self.mid_dim, self.action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        # Original initial layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Original residual layers\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.hw1(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "model_NN1 = Net()\n",
    "print(model_NN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ve3uwKv3pSw"
   },
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBgkfxwmwBgc"
   },
   "source": [
    "## 3.1 Set constants and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oh0fsz7JMcs"
   },
   "outputs": [],
   "source": [
    "# Set constants\n",
    "batch_size=16\n",
    "epochs=300\n",
    "\n",
    "# Reinitiating data here\n",
    "data = data_ohlcv\n",
    "\n",
    "X = data[['open', 'high', 'low', 'close', 'volume', 'rsi', 'macd', 'macd_signal', 'macd_hist', 'cci', 'dx', 'volatility']].values\n",
    "y = np.squeeze(data[['label_barrier']].values).astype(int)\n",
    "\n",
    "# Split into train+val and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)\n",
    "\n",
    "# Normalize input\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# initialize sets and convet them to Pytorch dataloader sets\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train.astype(int)).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test.astype(int)).long())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size\n",
    "                          )\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsDj1N0Zh1ON"
   },
   "source": [
    "## 3.2 Check if GPU availabble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKUIcIAZ7fb4",
    "outputId": "47a87d9e-e8e6-4e6c-a71a-95f7456359cf"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_NN1.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdAUenBzav9s",
    "outputId": "c45448ca-5902-4548-8210-3a1a9481ac22"
   },
   "outputs": [],
   "source": [
    "## Make sure you are working on GPU\n",
    "assert torch.cuda.is_available(), \"Change your runtime to GPU! Currently working on CPU... zzzzz\"\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRwOVdhpUoHi"
   },
   "source": [
    "## 3.3 Now train the ```model_NN1```; as long as the test loss reduces keep on training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCCUHYJM3EOO"
   },
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(fold, model, device, trainloader, optimizer, epoch):\n",
    "  model.train()\n",
    "  correct_train = 0\n",
    "  correct_this_batch_train = 0\n",
    "  total_train_loss = 0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      train_loss = criterion(output, target.flatten())\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if batch_idx % 100 == 0:\n",
    "          print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\ttrain_loss: {:.6f}'.format(\n",
    "              fold,epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "              100. * batch_idx / len(train_loader), train_loss.item()))\n",
    "          \n",
    "      # Measure accuracy on train set\n",
    "      total_train_loss += train_loss.item()\n",
    "      _, y_pred_tags_train = torch.max(output, dim = 1)\n",
    "      correct_this_batch_train = y_pred_tags_train.eq(target.flatten().view_as(y_pred_tags_train))\n",
    "      correct_train += correct_this_batch_train.sum().item()\n",
    "  \n",
    "  return correct_train, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw2Fd1hq3s5D"
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(fold,model, device, test_loader, correct_train, train_loss):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "          data, target = data.to(device), target.to(device)\n",
    "          output = model(data)\n",
    "          test_loss += criterion(output, target.flatten()).item()  # sum up batch loss\n",
    "\n",
    "          # Measure accuracy on test set\n",
    "          _, y_pred_tags = torch.max(output, dim = 1)\n",
    "          correct_this_batch = y_pred_tags.eq(target.flatten().view_as(y_pred_tags))\n",
    "          correct += correct_this_batch.sum().item() \n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  train_loss /= len(train_loader.dataset)\n",
    "\n",
    "  # Print train accuracy for epoch\n",
    "  # TODO: still a bug in summed up batch train loss \n",
    "  print('\\nTrain set for fold {}: Average train_loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)'.format(\n",
    "  fold, train_loss, correct_train, len(train_loader.dataset),\n",
    "  100 * correct_train / len(train_loader.dataset)))\n",
    "\n",
    "  # Print test result for epoch\n",
    "  print('Test set for fold {}:  Average test_loss:  {:.4f}, Accuracy: {}/{} ({:.5f}%)\\n'.format(\n",
    "      fold, test_loss, correct, len(test_loader.dataset),\n",
    "      100 * correct / len(test_loader.dataset)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Itp5eg4yxhU",
    "outputId": "55d40956-26d0-401a-e7b6-cb19e3927fb8"
   },
   "outputs": [],
   "source": [
    "model_NN1.to(device)\n",
    "\n",
    "# State fold (no PurgedKFold build yet, ignore this)\n",
    "# took about 1hour to train when epochs=300\n",
    "\n",
    "epochs=100\n",
    "fold = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "  correct_train, train_loss = train(fold, model_NN1, device, train_loader, optimizer, epoch)\n",
    "  test(fold, model_NN1, device, test_loader, correct_train, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ndn_gLUxPWFc",
    "outputId": "1ee9ecf8-a098-4a45-a42f-4e5caaf36a02"
   },
   "outputs": [],
   "source": [
    "# Save model to disk and save in your own files to save you some time\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "filename = 'model_NN1'\n",
    "out = open(filename, 'wb')\n",
    "\n",
    "with open(filename + '.pkl', 'wb') as fid:\n",
    "  pickle.dump(model_NN1, fid)\n",
    "\n",
    "# load pickle file\n",
    "with open(filename + '.pkl', 'rb') as fid:\n",
    "     model_NN1 = pickle.load(fid)\n",
    "\n",
    "files.download(filename + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTpeMX-zzTlU",
    "outputId": "4c068191-04d3-4c2d-f22c-b2a377a796db"
   },
   "outputs": [],
   "source": [
    "# load pickle filetorch.from_numpy(y_test.astype(int)).long()\n",
    "\n",
    "filename = 'model_NN1'\n",
    "with open(filename + '.pkl', 'rb') as fid:\n",
    "     model_NN1_pickle = pickle.load(fid)\n",
    "model_NN1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8sdW9bVwPeh"
   },
   "source": [
    "## 3.4 Get classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1lBRHR97D4-",
    "outputId": "c4587227-d949-4b4f-96b0-b2ba33d24d67"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # Show accuracy on test set\n",
    "  model_NN1.eval()\n",
    "\n",
    "  # predict proba\n",
    "  y_pred_nn1_proba = model_NN1(torch.from_numpy(X_test).float().to(device))\n",
    "  y_pred_nn1 = torch.argmax(y_pred_nn1_proba, dim=1)\n",
    "  y_pred_nn1 = y_pred_nn1.cpu().detach().numpy()\n",
    "\n",
    "# print predction values\n",
    "print('labels in prediction:', np.unique(y_pred_nn1), '\\n')\n",
    "\n",
    "# print report\n",
    "label_names = ['long', 'no bet', 'short']\n",
    "print(classification_report(y_test.astype(int), y_pred_nn1, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eT9tZiggKNQ",
    "outputId": "3c468314-963d-4a6d-c009-b0e8d12b22cd"
   },
   "outputs": [],
   "source": [
    "np.bincount(y_pred_nn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__1B0n8s4xhs"
   },
   "source": [
    "# Part 4: Feature Importance Analysis\n",
    "\n",
    "After we have a working neural network model (up to 66% accuracy with this network size), we can do a pertubation of the columns and do a prediction. When a column is pertubated and it delivers the highest error, that means that column in most important for the prediction of the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxxH6ZasFthF"
   },
   "source": [
    "## Pertubation Rank (PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "as0aJEnv1gSP"
   },
   "outputs": [],
   "source": [
    "def perturbation_rank(model,x,y,names):\n",
    "    errors = []\n",
    "\n",
    "    X_saved = x\n",
    "    y = y.flatten()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i in range(x.shape[1]):\n",
    "\n",
    "            # Convert to numpy, shuffle, convert back to tensor, predict\n",
    "            x = x.detach().numpy()\n",
    "            np.random.shuffle(x[:,i])\n",
    "            x = torch.from_numpy(x).float().to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            # log_loss requires (classification target, probabilities)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            error = metrics.log_loss(y, pred)\n",
    "            errors.append(error)\n",
    "\n",
    "            # Reset x to saved tensor matrix\n",
    "            x = X_saved\n",
    "    \n",
    "    max_error = np.max(errors)\n",
    "    importance = [e/max_error for e in errors]\n",
    "    \n",
    "    data = {'name':names,'error':errors,'importance':importance}\n",
    "    result = pd.DataFrame(data,columns = ['name','error','importance'])\n",
    "    result.sort_values(by=['importance'],ascending=[0],inplace=True)\n",
    "    result.reset_index(inplace=True,drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbGA923ZiPzv",
    "outputId": "9579a0c0-b882-48eb-be30-f62850d1c50b"
   },
   "outputs": [],
   "source": [
    "model_NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "tclwZN-fjXED",
    "outputId": "85bd7b60-b3ab-481a-de39-2587745d940f"
   },
   "outputs": [],
   "source": [
    "names = list(data_ohlcv.columns)\n",
    "names.remove('label_barrier')\n",
    "rank = perturbation_rank(model_NN1, \n",
    "                         torch.from_numpy(X_test).float(),\n",
    "                         torch.from_numpy(y_test.astype(int)).long(),  \n",
    "                         names\n",
    "                         )\n",
    "\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "dynI8Dd6hiv1",
    "outputId": "acffb113-31a6-4f73-b6f6-7f7bda8530f3"
   },
   "outputs": [],
   "source": [
    "names = list(data_ohlcv.columns)\n",
    "names.remove('label_barrier')\n",
    "rank = perturbation_rank(model_NN1, \n",
    "                         torch.from_numpy(X_test).float(),\n",
    "                         torch.from_numpy(y_test.astype(int)).long(),  \n",
    "                         names\n",
    "                         )\n",
    "\n",
    "display(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJOhQjyajKf_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Crypto_Feature_Importance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
